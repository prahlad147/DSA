Algorithm:
-> An algorithm is a set of well-defined instructions in sequence to solve a problem.




Qualities of Good Algorithms:

-> Input and output should be defined precisely.
-> Each step in the algorithm should be clear and unambiguous.
-> Algorithms should be most effective among many different ways to solve a problem.
-> An algorithm shouldn't include computer code. Instead, the algorithm should be written in such a way that it can be used in different 
      programming languages.
      
      
      

*********** Time and Space Complexity ***********
-> Time Complexity -> Lesser time involved.
-> Space Complexity -> Fewer memory involved.


-> Two of the most valuable resources for a computer program are time and memory.
-> The time taken by the computer to run code is: 
   Time to run code = number of instructions * time to execute each instruction
-> The number of instructions depends on the code you used, and the time taken to execute each code depends on your machine and compiler.




********** Code ***********
-> If it was written in a programming language, we would call it to code instead.



********** Scalability **********
-> Scalability is scale plus ability, which means the quality of an algorithm/system to handle the problem of larger size.




************ Asymptotic Analysis? **********

-> The efficiency of an algorithm depends on the amount of time, storage and other resources required to execute the algorithm. 
-> The efficiency is measured with the help of asymptotic notations.
-> The study of change in performance of the algorithm with the change in the order of the input size is defined as asymptotic analysis.




*********** Asymptotic Notations? ***********

-> Asymptotic notations are the mathematical notations used to describe the running time of an algorithm when the input tends towards a particular value or a 
limiting value.



There are mainly three asymptotic notations:
• Big-O notation
• Omega notation
• Theta notation




********** Big-O Notation (O-notation)? ***********

-> Big-O notation represents the upper bound of the running time of an algorithm. 
-> Thus, it gives the worst-case complexity of an algorithm.

-> O gives the upper bound of a function

      O(g(n)) = { f(n): there exist positive constants c and n0 such that 0 ≤ f(n) ≤ cg(n) for all n ≥ n0 }

-> The above expression can be described as a function f(n) belongs to the set O(g(n)) if there exists a positive constant c such that it lies between 0 and cg(n), 
   for sufficiently large n.

-> For any value of n, the running time of an algorithm does not cross the time provided by O(g(n)).

-> Since it gives the worst-case running time of an algorithm, it is widely used to analyze an algorithm as we are always interested in the worst-case scenario.




*********** Omega Notation (Ω-notation)? ************

-> Omega notation represents the lower bound of the running time of an algorithm. 
-> Thus, it provides the best case complexity of an algorithm.

-> Ω gives the lower bound of a function

      Ω(g(n)) = { f(n): there exist positive constants c and n0 such that 0 ≤ cg(n) ≤ f(n) for all n ≥ n0 }

-> The above expression can be described as a function f(n) belongs to the set Ω(g(n)) if there exists a positive constant c such that it lies above cg(n), for 
   sufficiently large n.

-> For any value of n, the minimum time required by the algorithm is given by Omega Ω(g(n)).




********** Theta Notation (Θ-notation)? ************

-> Theta notation encloses the function from above and below. 
-> Since it represents the upper and the lower bound of the running time of an algorithm, it is used for 
   analyzing the average-case complexity of an algorithm.

-> Theta bounds the function within constants factors.

-> For a function g(n), Θ(g(n)) is given by the relation:
      
      Θ(g(n)) = { f(n): there exist positive constants c1, c2 and n0 such that 0 ≤ c1g(n) ≤ f(n) ≤ c2g(n) for all n ≥ n0 }

-> The above expression can be described as a function f(n) belongs to the set Θ(g(n)) if there exist positive constants c1 and c2 such that it can be sandwiched 
   between c1g(n) and c2g(n), for sufficiently large n.

-> If a function f(n) lies anywhere in between c1g(n) and c2g(n) for all n ≥ n0, then 
   f(n) is said to be asymptotically tight bound.




********** Master Theorem? **********

-> The master method is a formula for solving recurrence relations of the form:
   
   T(n) = aT(n/b) + f(n),
where,
n = size of input
a = number of subproblems in the recursion
n/b = size of each subproblem. All subproblems are assumed to have the same size.
f(n) = cost of the work done outside the recursive call, which includes the cost of dividing the problem and cost of merging the solutions.
 
 
-> Here, a ≥ 1 and b > 1 are constants, and f(n) is an asymptotically positive function.

-> An asymptotically positive function means that for a sufficiently large value of n,  we have f(n) > 0.




-> The master theorem is used in calculating the time complexity of recurrence relations (divide and conquer algorithms) in a simple and quick way.

-> If a ≥ 1 and b > 1 are constants and f(n) is an asymptotically positive function, then the time complexity of a recursive relation is given by
   
   T(n) = aT(n/b) + f(n)

where, T(n) has the following asymptotic bounds:
 1. If f(n) = O(nlogb a-ϵ), then T(n) = Θ(nlogb a).
 2. If f(n) = Θ(nlogb a), then T(n) = Θ(nlogb a * log n).
 3. If f(n) = Ω(nlogb a+ϵ), then T(n) = Θ(f(n)).
ϵ > 0 is a constant.


-> Each of the above conditions can be interpreted as:
1. If the cost of solving the sub-problems at each level increases by a certain factor, the value of f(n) will become polynomially smaller than nlogb a. 
Thus, the time complexity is oppressed by the cost of the last level ie. nlogb a

2. If the cost of solving the sub-problem at each level is nearly equal, then the value of f(n) will be nlogb a. Thus, the time complexity will be f(n) times the 
total number of levels ie. nlogb a * log n.

3. If the cost of solving the subproblems at each level decreases by a certain factor, the value of f(n) will become polynomially larger than nlogb a. Thus, 
the time complexity is oppressed by the cost of f(n).




-> Solved Example of Master Theorem

T(n) = 3T(n/2) + n2
Here,
a = 3
n/b = n/2
f(n) = n2
logb a = log2 3 ≈ 1.58 < 2
ie. f(n) < nlogb
a+ϵ , where, ϵ is a constant.
Case 3 implies here.
Thus, T(n) = f(n) = Θ(n2) 
Master Theorem Limitations




-> The master theorem cannot be used if:

   • T(n) is not monotone. eg. T(n) = sin n
   • f(n) is not a polynomial. eg. f(n) = 2n
   • a is not a constant. eg. a = 2n
   • a < 1




********** Divide and Conquer Algorithm **********

-> A divide and conquer algorithm is a strategy of solving a large problem by

   1. breaking the problem into smaller sub-problems
   2. solving the sub-problems, and
   3. combining them to get the desired output.

-> To use the divide and conquer algorithm, recursion is used.




********** How Divide and Conquer Algorithms Work? **********

-> Here are the steps involved:
1. Divide: Divide the given problem into sub-problems using recursion.
2. Conquer: Solve the smaller sub-problems recursively. If the subproblem is small enough, then solve it directly.
3. Combine: Combine the solutions of the sub-problems that are part of the recursive process to solve the actual problem.


********** Divide and Conquer Vs Dynamic approach ***********

-> The divide and conquer approach divides a problem into smaller subproblems; these subproblems are further solved recursively. The result of each subproblem 
   is not stored for future reference, whereas, in a dynamic approach, the result of each subproblem is stored for future reference.
   
-> Use the divide and conquer approach when the same subproblem is not solved multiple times. 
-> Use the dynamic approach when the result of a subproblem is to be used multiple times in the future.






********** Advantages of Divide and Conquer Algorithm **********

   • The complexity for the multiplication of two matrices using the naive method is O(n^3), whereas using the divide and conquer approach (i.e. 
      Strassen's matrix multiplication) is O(n^2.8074). This approach also simplifies other problems, such as the Tower of Hanoi.
   • This approach is suitable for multiprocessing systems.
   • It makes efficient use of memory caches.


*********** Divide and Conquer Applications ***********

   • Binary Search
   • Merge Sort
   • Quick Sort
   • Strassen's Matrix multiplication
   • Karatsuba Algorithm




*********** Stack Data Structure ************

-> A stack is a useful data structure in programming. A stack is an object (an abstract data type).
-> It is just like a pile of plates kept on top of each other.
-> Stack representation similar to a pile of plate.


-> Think about the things you can do with such a pile of plates
• Put a new plate on top
• Remove the top plate


-> If you want the plate at the bottom, you must first remove all the plates on top. 
-> Such an arrangement is called Last In First Out - the last item that is the first item to go out.




************ LIFO Principle of Stack **********

-> In programming terms, putting an item on top of the stack is called push and removing an item is called pop.




*********** Basic Operations of Stack ***********

-> A stack is an object (an abstract data type - ADT) that allows the following operations:

   • Push: Add an element to the top of a stack
   • Pop: Remove an element from the top of a stack
   • IsEmpty: Check if the stack is empty
   • IsFull: Check if the stack is full
   • Peek: Get the value of the top element without removing it




********** Stack Time Complexity ***********

-> For the array-based implementation of a stack, the push and pop operations take constant time, i.e. O(1)




********** Applications of Stack Data Structure **********

-> Although stack is a simple data structure to implement, it is very powerful. 

-> The most common uses of a stack are:

   • To reverse a word - Put all the letters in a stack and pop them out. Because of the LIFO order of stack, you will get the letters in reverse order.
   • In compilers - Compilers use the stack to calculate the value of expressions like 2 + 4 / 5 * (7 - 9) by converting the expression to prefix or postfix form.
   • In browsers - The back button in a browser saves all the URLs you have visited previously in a stack. Each time you visit a new page, it is added on 
      top of the stack. When you press the back button, the current URL is removed from the stack, and the previous URL is accessed.




*********** Queue Data Structure **********

-> A queue is a useful data structure in programming. A queue is an object (an abstract data type - ADT).

-> It is similar to the ticket queue outside a cinema hall, where the first person entering the queue is the first person who gets the ticket.


-> Queue follows the First In First Out (FIFO) rule - the item that goes in first is the item that comes out first.




********** FIFO Principle of Queue ***********

-> In programming terms, putting items in the queue is called enqueue, and removing items from the queue is called dequeue.




********** Basic Operations of Queue **********

-> A queue is an object (an abstract data structure - ADT) that allows the following operations:

   • Enqueue: Add an element to the end of the queue
   • Dequeue: Remove an element from the front of the queue
   • IsEmpty: Check if the queue is empty
   • IsFull: Check if the queue is full
   • Peek: Get the value of the front of the queue without removing it




*********** Complexity Analysis of Queue Operations ***********

-> The complexity of enqueue and dequeue operations in a queue using an array is O(1).




*********** Applications of Queue ************

   • CPU scheduling, Disk Scheduling
   • When data is transferred asynchronously between two processes. The queue is used for synchronization. For example: IO Buffers, pipes, file IO, etc.
   • Handling of interrupts in real-time systems.
   • Call Center phone systems use Queues to hold people calling them in order.




********** Types of Queues ************

There are four different types of queues:
• Simple Queue
• Circular Queue
• Priority Queue
• Double Ended Queue




*********** Simple Queue ***********

-> In a simple queue, insertion takes place at the rear and removal occurs at the front. 
-> It strictly follows the FIFO (First in First out) rule.




************ Circular Queue ***********

-> In a circular queue, the last element points to the first element making a circular link.




************ Simple Queue Vs Circular Queue ***********

-> The main advantage of a circular queue over a simple queue is better memory utilization. 
-> If the last position is full and the first position is empty, we can insert an element in the first position. 
-> This action is not possible in a simple queue.




*********** Priority Queue ************

-> A priority queue is a special type of queue in which each element is associated with a priority and is served according to its priority. 
-> If elements with the same priority occur, they are served according to their order in the queue.
-> Insertion occurs based on the arrival of the values and removal occurs based on priority.




************ Deque (Double Ended Queue) ************

-> In a double ended queue, insertion and removal of elements can be performed from either from the front or rear. 
-> Thus, it does not follow the FIFO (First In First Out) rule.




************ Circular Queue Data Structure ************

-> Circular queue avoids the wastage of space in a regular queue implementation using arrays.




************ How Circular Queue Works ************

-> Circular Queue works by the process of circular increment i.e. when we try to increment the pointer and we reach the end of the queue, we start from the 
   beginning of the queue.

-> Here, the circular increment is performed by modulo division with the queue size. 

-> That is,
      if REAR + 1 == 5 (overflow!), REAR = (REAR + 1)%5 = 0 (start of queue).
      
      
      
      
************ Circular Queue Complexity Analysis ************

-> The complexity of the enqueue and dequeue operations of a circular queue is O(1) for (array implementations).




************* Applications of Circular Queue ************

   • CPU scheduling
   • Memory management
   • Traffic Management


************ Priority Queue ***********

-> A priority queue is a special type of queue in which each element is associated with a priority and is served according to its priority. 
-> If elements with the same priority occur, they are served according to their order in the queue.

-> Generally, the value of the element itself is considered for assigning the priority.
-> For example, The element with the highest value is considered as the highest 
   priority element. 




************ Difference between Priority Queue and Normal Queue ***********

-> In a queue, the first-in-first-out rule is implemented whereas, in a priority queue, the values are removed on the basis of priority. 
-> The element with the highest priority is removed first.




*********** Implementation of Priority Queue ***********

-> Priority queue can be implemented using an array, a linked list, a heap data structure, or a binary search tree. 
-> Among these data structures, heap data structure provides an efficient implementation of priority queues.

-> A comparative analysis of different implementations of priority queue is given below.

   Operations          peek  insert    delete
   Linked List         O(1)  O(n)      O(1)
   Binary Heap         O(1)  O(log n)  O(log n)
   Binary Search Tree  O(1)  O(log n)  O(log n)




************ Priority Queue Operations ***********

-> Basic operations of a priority queue are inserting, removing, and peeking elements


1. Inserting an Element into the Priority Queue

Inserting an element into a priority queue (max-heap) is done by the following steps.
   
   • Insert the new element at the end of the tree.  
   • Insert an element at the end of the queue 
   • Heapify the tree. 
   • Heapify after insertion




2. Deleting an Element from the Priority Queue

Deleting an element from a priority queue (max-heap) is done as follows:

   • Select the element to be deleted. 
   • Swap it with the last element. 
   • Swap with the last leaf node element 
   • Remove the last element. 
   • Remove the last element leaf 
   • Heapify the tree. 
   • Heapify the priority queue 




3. Peeking from the Priority Queue (Find max/min)

Peek operation returns the maximum element from Max Heap or minimum element from Min Heap without deleting the node.




4. Extract-Max/Min from the Priority Queue

Extract-Max returns the node with maximum value after removing it from a Max Heap whereas Extract-Min returns the node with minimum value after removing it 
from Min Heap.




*********** Priority Queue Applications ************

Some of the applications of a priority queue are:
   • Dijkstra's algorithm
   • for implementing stack
   • for load balancing and interrupt handling in an operating system
   • for data compression in Huffman code




*********** Deque Data Structure ***********

-> Deque or Double Ended Queue is a type of queue in which insertion and removal of elements can be performed from either from the front or rear. 
-> Thus, it does not follow FIFO rule (First In First Out).




*********** Types of Deque ***********

   • Input Restricted Deque
   In this deque, input is restricted at a single end but allows deletion at both the ends.
   • Output Restricted Deque
   In this deque, output is restricted at a single end but allows insertion at both the ends.




*********** Operations on a Deque ************

-> Below is the circular array implementation of deque. In a circular array, if the array is full, we start from the beginning.
-> But in a linear array implementation, if the array is full, no more elements can be inserted. 
-> In each of the operations below, if the array is full, "overflow message" is thrown.




*********** Time Complexity ************

-> The time complexity of all the above operations is constant i.e. O(1).




*********** Applications of Deque Data Structure ************

1. In undo operations on software.
2. To store history in browsers.
3. For implementing both stacks and queues




********** Linked list Data Structure **********

-> A linked list data structure includes a series of connected nodes. 
-> Here, each node store the data and the address of the next node.


-> You have to start somewhere, so we give the address of the first node a special name called HEAD.
-> Also, the last node in the linked list can be identified because its next portion points to NULL.


-> You might have played the game Treasure Hunt, where each clue includes the information about the next clue. That is how the linked list operates.


********** Representation of Linked List **********

Each node consists:
• A data item
• An address of another node.




-> The power of a linked list comes from the ability to break the chain and rejoin it. 

E.g. if you wanted to put an element 4 between 1 and 2, the steps would be:
• Create a new struct node and allocate memory to it.
• Add its data value as 4
• Point its next pointer to the struct node containing 2 as the data value
• Change the next pointer of "1" to the node we just created.




*********** Linked List Complexity ***********

Time Complexity

Operations  Worst case  Average Case
Search      O(n)        O(n)
Insert      O(1)        O(1)
Deletion    O(1)        O(1)


Space Complexity: O(n)




********** Linked List Applications **********

• Dynamic memory allocation
• Implemented in stack and queue
• In undo functionality of softwares
• Hash tables, Graphs




*********** Linked List Operations: Traverse, Insert and Delete **********

Two important points to remember:
• head points to the first node of the linked list
• next pointer of the last node is NULL, so if the next current node is NULL, we have reached the end of the linked list.




********** Traverse a Linked List **********

-> Displaying the contents of a linked list is very simple.
-> We keep moving the temp node to the next one and display its contents.
-> When temp is NULL, we know that we have reached the end of the linked list so we get out of the while loop.




********** Insert Elements to a Linked List **********

-> You can add elements to either the beginning, middle or end of the linked list.


1. Insert at the beginning
• Allocate memory for new node
• Store data
• Change next of new node to point to head
• Change head to point to recently created node


2. Insert at the End
• Allocate memory for new node
• Store data
• Traverse to last node
• Change next of last node to recently created node


3. Insert at the Middle
• Allocate memory and store data for new node
• Traverse to node just before the required position of new node
• Change next pointers to include new node in between




********** Delete from a Linked List **********

-> You can delete either from the beginning, end or from a particular position.


1. Delete from beginning
• Point head to the second node


2. Delete from end
• Traverse to second last element
• Change its next pointer to null


3. Delete from middle
• Traverse to element before the element to be deleted
• Change next pointers to exclude the node from the chain




********** Types of Linked List **********

There are three common types of Linked List.
• Singly Linked List
• Doubly Linked List
• Circular Linked List




********** Singly Linked List **********

-> It is the most common. Each node has data and a pointer to the next node.




********** Doubly Linked List **********

-> We add a pointer to the previous node in a doubly-linked list. Thus, we can go in either direction: forward or backward.




********** Circular Linked List **********

-> A circular linked list is a variation of a linked list in which the last element is linked to the first element. 
-> This forms a circular loop.


-> A circular linked list can be either singly linked or doubly linked.
• for singly linked list, next pointer of last item points to the first item.
• In the doubly linked list, prev pointer of the first item points to the last item as well.




************ Hash Table **********

-> The Hash table data structure stores elements in key-value pairs where
• Key- unique integer that is used for indexing the values
• Value - data that are associated with keys.




*********** Hashing **********

-> Hashing is a technique of mapping a large set of arbitrary data to tabular indexes using a hash function. 
-> It is a method for representing dictionaries for large datasets.
-> It allows lookups, updating and retrieval operation to occur in a constant time i.e. O(1).




*********** Why Hashing is Needed **********

-> After storing a large amount of data, we need to perform various operations on these data. 
-> Lookups are inevitable for the datasets. 
-> Linear search and binary search perform lookups/search with time complexity of O(n) and O(log n) respectively. 
-> As the size of the dataset increases, these complexities also become significantly high which is not acceptable.

-> We need a technique that does not depend on the size of data. 
-> Hashing allows lookups to occur in constant time i.e. O(1)




*********** Hash Function **********

-> A hash function is used for mapping each element of a dataset to indexes in the table.

-> In a hash table, a new index is processed using the keys. 
-> And, the element corresponding to that key is stored in the index. This process is called hashing.

-> Let k be a key and h(x) be a hash function.
-> Here, h(k) will give us a new index to store the element linked with k.




*********** Hash Collision ***********

-> When the hash function generates the same index for multiple keys, there will be a conflict (what value to be stored in that index). 
-> This is called a hash collision.


-> We can resolve the hash collision using one of the following techniques.
• Collision resolution by chaining
• Open Addressing: Linear/Quadratic Probing and Double Hashing




1. Collision resolution by chaining
-> In chaining, if a hash function produces the same index for multiple elements, these elements are stored in the same index by using a doubly-linked list.
-> If j is the slot for multiple elements, it contains a pointer to the head of the list of elements. If no element is present, j contains NIL.

2. Open Addressing
-> Unlike chaining, open addressing doesn't store multiple elements into the same slot. 
-> Here, each slot is either filled with a single key or left NIL.


Different techniques used in open addressing are:

i. Linear Probing
-> In linear probing, collision is resolved by checking the next slot.
   
   h(k, i) = (h′(k) + i) mod m
   where,
   • i = {0, 1, ….}
   • h'(k) is a new hash function

-> If a collision occurs at h(k, 0), then h(k, 1) is checked. 
-> In this way, the value of i is incremented linearly.


-> The problem with linear probing is that a cluster of adjacent slots is filled. 
-> When inserting a new element, the entire cluster must be traversed. 
-> This adds to the time required to perform operations on the hash table.




ii. Quadratic Probing
-> It works similar to linear probing but the spacing between the slots is increased (greater than one) by using the following relation.

   h(k, i) = (h′(k) + c1i + c2i^2) mod m
   where,
   • c1 and c2 are positive auxiliary constants,
   • i = {0, 1, ….}




iii. Double hashing
-> If a collision occurs after applying a hash function h(k), then another hash function is calculated for finding the next slot.

   h(k, i) = (h1(k) + ih2(k)) mod m




*********** Good Hash Functions **********

-> A good hash function may not prevent the collisions completely however it can reduce the number of collisions.



-> Here, we will look into different methods to find a good hash function

1. Division Method
-> If k is a key and m is the size of the hash table, the hash function h() is calculated as:

   h(k) = k mod m

-> For example, If the size of a hash table is 10 and k = 112 then h(k) = 112 mod 10 = 2. 
-> The value of m must not be the powers of 2. This is because the powers of 2 in binary format are 10, 100, 1000, …. 
-> When we find k mod m, we will always get the lower order p-bits.
   if m = 22, k = 17, then h(k) = 17 mod 22 = 10001 mod 100 = 01
   if m = 23, k = 17, then h(k) = 17 mod 22 = 10001 mod 100 = 001
   if m = 24, k = 17, then h(k) = 17 mod 22 = 10001 mod 100 = 0001
   if m = 2p, then h(k) = p lower bits of m




2. Multiplication Method

   h(k) = ⌊m(kA mod 1)⌋
   where,
   • kA mod 1 gives the fractional part kA,
   • ⌊ ⌋ gives the floor value
   • A is any constant. The value of A lies between 0 and 1. But, an optimal choice will be ≈ (√5-1)/2 suggested by Knuth.




3. Universal Hashing

-> In Universal hashing, the hash function is chosen at random independent of keys




********** Applications of Hash Table **********

-> Hash tables are implemented where
• constant time lookup and insertion is required
• cryptographic applications
• indexing data is required




********** Heap Data Structure **********

-> Heap data structure is a complete binary tree that satisfies the heap property. It is also called as a binary heap.

-> A complete binary tree is a special binary tree in which
• every level, except possibly the last, is filled
• all the nodes are as far left as possible




*********** Heap Property **********

-> Heap Property is the property of a node in which:

• (for max heap) key of each node is always greater than its child node/s and the key of the root node is the largest among all other nodes;

• (for min heap) key of each node is always smaller than the child node/s and the key of the root node is the smallest among all other nodes.




********** Heap Operations **********

-> Some of the important operations performed on a heap are described below along with their algorithms: Heapify




********** Heapify **********

->Heapify is the process of creating a heap data structure from a binary tree. It is used to create a Min-Heap or a Max-Heap.

   1. Let the input array.
   2. Create a complete binary tree from the array
   3. Start from the first index of non-leaf node whose index is given by n/2 - 1.
   4. Set current element i as largest.
   5. The index of left child is given by 2i + 1 and the right child is given by 2i + 2.
      If leftChild is greater than currentElement (i.e. element at ith index), set leftChildIndex as largest.
      If rightChild is greater than element in largest, set rightChildIndex as largest.
   6. Swap largest with currentElement
   7. Repeat steps 3-7 until the subtrees are also heapified.





*********** Insert Element into Max Heap **********

1. Insert the new element at the end of the tree.
2. Heapify the tree.

-> For Min Heap, the above algorithm is modified so that parentNode is always smaller than newNode.


********** Delete Element from Max Heap **********

1. Select the element to be deleted.
2. Swap it with the last element.
3. Remove the last element.
4. Heapify the tree.

-> For Min Heap, above algorithm is modified so that both childNodes are greater smaller than currentNode.




*********** Peek (Find max/min) **********

-> Peek operation returns the maximum element from Max Heap or minimum element from Min Heap without deleting the node.

-> For both Max heap and Min Heap

   return rootNode




********** Extract-Max/Min **********

-> Extract-Max returns the node with maximum value after removing it from a Max Heap.
-> whereas Extract-Min returns the node with minimum after removing it from Min Heap.




*********** Heap Data Structure Applications **********

• Heap is used while implementing a priority queue.
• Dijkstra’s Algorithm
• Heap Sort




********** Tree Data Structure **********

A tree is a nonlinear hierarchical data structure that consists of nodes connected by edges.




********** Linear Vs Non-Linear Data Structure ***********

-> In linear data structure, all data elements are present at a single level. 
-> Linear data structures are easier to implement.

-> In non-linear data structure, data elements are hierarchically connected and are present at various levels. 
-> In non-linear data structure, data elements are present at multiple levels.




********** Why Tree Data Structure? **********

-> Other data structures such as arrays, linked list, stack, and queue are linear data structures that store data sequentially. 
-> In order to perform any operation in a linear data structure, the time complexity increases with the increase in the data size. 
-> But, it is not acceptable in today's computational world.

-> Different tree data structures allow quicker and easier access to the data as it is a non-linear data structure.




********** Tree Terminologies **********

Node:

-> A node is an entity that contains a key or value and pointers to its child nodes.
-> The last nodes of each path are called leaf nodes or external nodes that do not contain a link/pointer to child nodes.
-> The node having at least a child node is called an internal node.



Edge:

-> It is the link between any two nodes.



Root:

-> It is the topmost node of a tree.


Height of a Node:

-> The height of a node is the number of edges from the node to the deepest leaf (ie. the longest path from the node to a leaf node).


Depth of a Node:

-> The depth of a node is the number of edges from the root to the node


Height of a Tree:

-> The height of a Tree is the height of the root node or the depth of the deepest node.


Degree of a Node:

-> The degree of a node is the total number of branches of that node.


Forest:

-> A collection of disjoint trees is called a forest.


Creating forest from a tree:

-> You can create a forest by cutting the root of a tree




********** Types of Tree **********

There are mainly four types of Tree:
1. Binary Tree
2. Binary Search Tree
3. AVL Tree
4. B-Tree




********** Tree Applications **********

   • Binary Search Trees(BSTs) are used to quickly check whether an element is present in a set or not.
   • Heap is a kind of tree that is used for heap sort.
   • A modified version of a tree called Tries is used in modern routers to store routing information.
   • Most popular databases use B-Trees and T-Trees, which are variants of the tree structure we learned above to store their data
   • Compilers use a syntax tree to validate the syntax of every program you write.




********** Tree Traversal - inorder, preorder and postorder **********

-> Traversing a tree means visiting every node in the tree. 
-> You might, for instance, want to add all the values in the tree or find the largest one. For all these operations, you will need to visit each node of the tree.


-> Linear data structures like arrays, stacks, queues, and linked list have only one way to read the data. 
-> But a hierarchical data structure like a tree can be traversed in different ways.


-> According to this structure, every tree is a combination of
• A node carrying data
• Two subtrees

-> Remember that our goal is to visit each node, so we need to visit all the nodes in the subtree, 
-> visit the root node and visit all the nodes in the right subtree as well.


-> Depending on the order in which we do this, there can be three types of traversal.


Inorder traversal:

1. First, visit all the nodes in the left subtree
2. Then the root node
3. Visit all the nodes in the right subtree


Preorder traversal:

1. Visit root node
2. Visit all the nodes in the left subtree
3. Visit all the nodes in the right subtree


Postorder traversal:

1. Visit all the nodes in the left subtree
2. Visit all the nodes in the right subtree
3. Visit the root node




********** Binary Tree **********

-> A binary tree is a tree data structure in which each parent node can have at most two children.




********** Types of Binary Tree ***********

Full Binary Tree:

-> A full Binary tree is a special type of binary tree in which every parent node/internal node has either two or no children.


Perfect Binary Tree:

-> A perfect binary tree is a type of binary tree in which every internal node has exactly two child nodes and all the leaf nodes are at the same level.



Complete Binary Tree:

-> A complete binary tree is just like a full binary tree, but with two major differences

   1. Every level must be completely filled
   2. All the leaf elements must lean towards the left.
   3. The last leaf element might not have a right sibling i.e. a complete binary tree doesn't have to be a full binary tree.



Degenerate or Pathological Tree:

-> A degenerate or pathological tree is the tree having a single child either left or right.


Skewed Binary Tree:

-> A skewed binary tree is a pathological/degenerate tree in which the tree is either dominated by the left nodes or the right nodes. 


Thus, there are two types of skewed binary tree: 
1.left-skewed binary tree.
2.right-skewed binary tree.



Balanced Binary Tree:

-> It is a type of binary tree in which the difference between the height of the left and the right subtree for each node is either 0 or 1.


********** Binary Tree Applications ***********

   • For easy and quick access to data
   • In router algorithms
   • To implement heap data structure
   • Syntax tree




********** Full Binary Tree **********

-> A full Binary tree is a special type of binary tree in which every parent node/internal node has either two or no children.
-> It is also known as a proper binary tree.




********** Full Binary Tree Theorems **********

   Let, i = the number of internal nodes
        n = be the total number of nodes
        l = number of leaves
        λ = number of levels


1. The number of leaves is i + 1.
2. The total number of nodes is 2i + 1.
3. The number of internal nodes is (n – 1) / 2.
4. The number of leaves is (n + 1) / 2.
5. The total number of nodes is 2l – 1.
6. The number of internal nodes is l – 1.
7. The number of leaves is at most 2^λ - 1.




********** Perfect Binary Tree **********

-> A perfect binary tree is a type of binary tree in which every internal node has exactly two child nodes and all the leaf nodes are at the same level.

-> All the internal nodes have a degree of 2.




-> Recursively, a perfect binary tree can be defined as:

1. If a single node has no children, it is a perfect binary tree of height h = 0,
2. If a node has h > 0, it is a perfect binary tree if both of its subtrees are of height h - 1 and are non-overlapping.




*********** Perfect Binary Tree Theorems **********

1. A perfect binary tree of height h has 2h + 1 – 1 node.
2. A perfect binary tree with n nodes has height log(n + 1) – 1 = Θ(ln(n)).
3. A perfect binary tree of height h has 2h leaf nodes.
4. The average depth of a node in a perfect binary tree is Θ(ln(n))




Complete Binary Tree:

-> A complete binary tree is a binary tree in which all the levels are completely filled except possibly the lowest one, which is filled from the left.

-> A complete binary tree is just like a full binary tree, but with two major differences
1. All the leaf elements must lean towards the left.
2. The last leaf element might not have a right sibling 
      i.e. a complete binary tree doesn't have to be a full binary tree.




Full Vs Complete Binary Trees:

-> A full binary tree (sometimes proper binary tree or 2-tree) is a tree in which every node other than the leaves has two children. 
-> A complete binary tree is a binary tree in which every level, except possibly the last, is completely filled, and all nodes are as far left as possible.




How a Complete Binary Tree is Created?

1. Select the first element of the list to be the root node. (no. of elements on level-I: 1)
2. Put the second element as a left child of the root node and the third element as the right child. (no. of elements on level-II: 2)
3. Put the next two elements as children of the left node of the second level. 
      Again, put the next two elements as children of the right node of the second level (no. of elements on level-III: 4) elements).
4. Keep repeating until you reach the last element.\




Relationship between array indexes and tree element:

-> A complete binary tree has an interesting property that we can use to find the children and parents of any node.
-> If the index of any element in the array is i, the element in the index 2i+1 will become the left child and element in 2i+2 index will become the right child. 
-> Also, the parent of any element at index i is given by the lower bound of (i-1)/2.




Complete Binary Tree Applications:
• Heap-based data structures
• Heap sort




Balanced Binary Tree:

-> A balanced binary tree, also referred to as a height-balanced binary tree, is defined as a binary tree in which
    the height of the left and right subtree of any node differ by not more than 1.


Following are the conditions for a height-balanced binary tree:
1. Difference between the left and the right subtree for any node is not more than one.
2. The left subtree is balanced.
3. The right subtree is balanced.



Balanced Binary Tree Applications:
• AVL tree
• Balanced Binary Search Tree



Binary Search Tree(BST):

-> Binary search tree is a data structure that quickly allows us to maintain a sorted list of numbers.
-> It is called a binary tree because each tree node has a maximum of two children.
-> It is called a search tree because it can be used to search for the presence of a number in O(log(n)) time.




The properties that separate a binary search tree from a regular binary tree is:
1. All nodes of left subtree are less than the root node.
2. All nodes of right subtree are more than the root node.
3. Both subtrees of each node are also BSTs i.e. they have the above two properties.




There are two basic operations that you can perform on a binary search tree:

1. Search Operation:

-> The algorithm depends on the property of BST that if each left subtree has values below root and each right subtree has values above the root.
-> If the value is below the root, we can say for sure that the value is not in the right subtree; we need to only search in the left subtree 
      and if the value is above the root, we can say for sure that the value is not in the left subtree; we need to only search in the right subtree.


Algorithm:
If root == NULL 
 return NULL;
If number == root -> data
 return root -> data;
If number < root -> data
 return search(root -> left)
If number > root -> data
 return search(root -> right)



2. Insert Operation:

-> Inserting a value in the correct position is similar to searching because we try to maintain 
      the rule that the left subtree is lesser than root and the right subtree is larger than root.
-> We keep going to either right subtree or left subtree depending on the value and when 
      we reach a point left or right subtree is null, we put the new node there.


Algorithm:
If node == NULL 
 return createNode(data)
if (data < node -> data)
 node -> left = insert(node -> left, data);
else if (data > node -> data)
 node -> right = insert(node -> right, data); 
return node;




Deletion Operation:

There are three cases for deleting a node from a binary search tree.

Case I:
-> In the first case, the node to be deleted is the leaf node. 
-> In such a case, simply delete the node from the tree.


Case II:
-> In the second case, the node to be deleted lies has a single child node. 
-> In such a case follow the steps below:
1. Replace that node with its child node.
2. Remove the child node from its original position.


Case III:
-> In the third case, the node to be deleted has two children. 
-> In such a case follow the steps below:
1. Get the inorder successor of that node.
2. Replace the node with the inorder successor.
3. Remove the inorder successor from its original position.




Binary Search Tree Complexities:

Time Complexity:

Operation     Best Case  Average Case  Worst Case
Search        O(log n)   O(log n)      O(n)
Insertion     O(log n)   O(log n)      O(n)
Deletion      O(log n)   O(log n)      O(n)

Here, n is the number of nodes in the tree.


Space Complexity:

-> The space complexity for all the operations is O(n).



Binary Search Tree Applications:
1. In multilevel indexing in the database
2. For dynamic sorting
3. For managing virtual memory areas in Unix kernel



Graph Data Stucture:

-> A graph data structure is a collection of nodes that have data and are connected to other nodes.

-> Let's try to understand this through an example. On facebook, everything is a node. 
-> That includes User, Photo, Album, Event, Group, Page, Comment, Story, Video, Link, Note...anything that has data is a node.


-> Every relationship is an edge from one node to another. 
-> Whether you post a photo, join a group, like a page, etc., a new edge is created for that relationship.


-> All of facebook is then a collection of these nodes and edges. 
-> This is because facebook uses a graph data structure to store its data.



More precisely, a graph is a data structure (V, E) that consists of:
• A collection of vertices V
• A collection of edges E, represented as ordered pairs of vertices (u,v)




In the graph,
V = {0, 1, 2, 3}
E = {(0,1), (0,2), (0,3), (1,2)}
G = {V, E}




Graph Terminology:

• Adjacency: 
-> A vertex is said to be adjacent to another vertex if there is an edge connecting them. 
-> Vertices 2 and 3 are not adjacent because there is no edge between them.

• Path: 
-> A sequence of edges that allows you to go from vertex A to vertex B is called a path. 
-> 0-1, 1-2 and 0-2 are paths from vertex 0 to vertex 2.

• Directed Graph: 
-> A graph in which an edge (u,v) doesn't necessarily mean that there is an edge (v, u) as well. 
-> The edges in such a graph are represented by arrows to show the direction of the edge.




Graph Representation:

Graphs are commonly represented in two ways:

1. Adjacency Matrix

-> An adjacency matrix is a 2D array of V x V vertices. 
-> Each row and column represent a vertex.
-> If the value of any element a[i][j] is 1, it represents that there is an edge connecting vertex i and vertex j.

-> Edge lookup (checking if an edge exists between vertex A and vertex B) is extremely fast in adjacency matrix representation but we have to reserve space for 
      every possible link between all vertices (V x V), so it requires more space.




Pros of adjacency matrix:

-> The basic operations like adding an edge, removing an edge and checking whether there is an edge from vertex i to vertex j 
      are extremely time efficient, constant time operations.
      
-> If the graph is dense and the number of edges is large, adjacency matrix should be the first choice. 
-> Even if the graph and the adjacency matrix is sparse, we can represent it using data structures for sparse matrices.

-> The biggest advantage however, comes from the use of matrices. 
-> The recent advances in hardware enable us to perform even expensive matrix operations on the GPU.

-> By performing operations on the adjacent matrix, we can get important insights into the nature of the graph and the relationship between its vertices.




Cons of adjacency matrix:

-> The VxV space requirement of the adjacency matrix makes it a memory hog. 

-> Graphs out in the wild usually don't have too many connections and this is the major reason why 
      adjacency lists are the better choice for most tasks.

-> While basic operations are easy, operations like inEdges and outEdges are expensive when using the adjacency matrix representation.



Adjacency Matrix Applications:
1. Creating routing table in networks
2. Navigation tasks




2. Adjacency List

-> An adjacency list represents a graph as an array of linked lists.
-> The index of the array represents a vertex and each element in its linked list represents the other vertices that form an edge with the vertex


-> An adjacency list is efficient in terms of storage because we only need to store the values for the edges. 
-> For a sparse graph with millions of vertices, this can mean a lot of saved space.


Adjacency List Structure:
-> The simplest adjacency list needs a node data structure to store a vertex and a graph data structure to organize the nodes.

-> We stay close to the basic definition of a graph - a collection of vertices and edges {V, E}. 
-> For simplicity, we use an unlabeled graph as opposed to a labeled one i.e. the vertices are identified by their indices 0,1,2,3.




Graph Operations:

The most common graph operations are:
• Check if the element is present in the graph.
• Graph Traversal.
• Add elements(vertex, edges) to graph.
• Finding the path from one vertex to another.




Note:
-> An undirected graph is a graph in which the edges do not point in any direction (ie. the edges are bidirectional).
-> A connected graph is a graph in which there is always a path from a vertex to any other vertex.
-> Dense graph is a graph in which the number of edges is close to the maximal number of edges. 
-> Sparse graph is a graph in which the number of edges is close to the minimal number of edges.



Spanning Tree and Minimum Spanning Tree:
-> A spanning tree is a sub-graph of an undirected connected graph, which includes all the vertices of the graph with a minimum possible number of edges. 
-> If a vertex is missed, then it is not a spanning tree.
-> The edges may or may not have weights assigned to them.


-> The total number of spanning trees with n vertices that can be created from a complete graph is equal to n^(n-2).


If we have n = 4, the maximum number of possible spanning trees is equal to 4^(4-2) = 16. 
Thus, 16 spanning trees can be formed from a complete graph with 4 vertices.




Minimum Spanning Tree:

-> A minimum spanning tree is a spanning tree in which the sum of the weight of the edges is as minimum as possible.


Notes:

The minimum spanning tree from a graph is found using the following algorithms:
1. Krusal's Algorithm
2. Prim's Algorithm



-> Kruskal's algorithm finds a minimum spanning forest of an undirected edge-weighted graph. 
-> If the graph is connected, it finds a minimum spanning tree. 
-> It is a greedy algorithm in graph theory as in each step it adds the next lowest-weight edge that will not form a cycle to the minimum spanning forest.


2. Prim's Algorithm:
-> Prim's algorithm is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph. 
-> This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized.




Spanning Tree Applications:
1. Computer Network Routing Protocol
2. Cluster Analysis
3. Civil Network Planning




Minimum Spanning tree Applications:
• To find paths in the map
• To design networks like telecommunication networks, water supply networks, and electrical grids.




Strongly Connected Components:

-> A strongly connected component is the portion of a directed graph in which there is a path from each vertex to another vertex. 
-> It is applicable only on a directed graph.
-> These components can be found using Kosaraju's Algorithm.


Strongly Connected Components Applications:
• Vehicle routing applications
• Maps
• Model-checking in formal verification


Kosaraju's Algorithm:

-> Kosaraju's Algorithm is based on the depth-first search algorithm implemented twice.

Three steps are involved:

1. Perform a depth first search on the whole graph.
-> Let us start from vertex-0, visit all of its child vertices, and mark the visited vertices as done. 
-> If a vertex leads to an already visited vertex, then push this vertex to the stack.

For example: Starting from vertex-0, go to vertex-1, vertex-2, and then to vertex-3. Vertex-3 leads to already visited vertex-0, 
      so push the source vertex (ie. vertex-3) into the stack.
      
      
2. Go to the previous vertex (vertex-2) and visit its child vertices i.e. vertex-4, vertex-5, vertex-6 and vertex-7 sequentially. 
      Since there is nowhere to go from vertex-7, push it into the stack.
   Go to the previous vertex (vertex-6) and visit its child vertices. 
      But, all of its child vertices are visited, so push it into the stack.
   Similarly, a final stack is created.


3. Reverse the original graph.

4. Perform depth-first search on the reversed graph.
-> Start from the top vertex of the stack. Traverse through all of its child vertices. 
-> Once the already visited vertex is reached, one strongly connected component is formed.

For example: Pop vertex-0 from the stack. Starting from vertex-0, traverse 
through its child vertices (vertex-0, vertex-1, vertex-2, vertex-3 in sequence) and mark them as visited. 
The child of vertex-3 is already visited, so these visited vertices form one strongly connected component.

Go to the stack and pop the top vertex if already visited. Otherwise, choose the top vertex from the stack and traverse through its child vertices as presented.
above.


5. Thus, we get the strongly connected components.




Kosaraju's Algorithm Complexity:
-> Kosaraju's algorithm runs in linear time i.e. O(V+E).


Depth First Search (DFS):

-> Depth first Search or Depth first traversal is a recursive algorithm for searching all the vertices of a graph or tree data structure. 
-> Traversal means visiting all the nodes of a graph.




Depth First Search Algorithm:
A standard DFS implementation puts each vertex of the graph into one of two categories:
1. Visited
2. Not Visited




The DFS algorithm works as follows:
1. Start by putting any one of the graph's vertices on top of a stack.
2. Take the top item of the stack and add it to the visited list.
3. Create a list of that vertex's adjacent nodes. 
      Add the ones which aren't in the visited list to the top of the stack.
4. Keep repeating steps 2 and 3 until the stack is empty




Depth First Search Example:

Let's see how the Depth First Search algorithm works with an example. 

We use an undirected graph with 5 vertices.

-> We start from vertex 0, the DFS algorithm starts by putting it in the Visited list and putting all its adjacent vertices in the stack.
-> Next, we visit the element at the top of stack i.e. 1 and go to its adjacent nodes. Since 0 has already been visited, we visit 2 instead.
-> Vertex 2 has an unvisited adjacent vertex in 4, so we add that to the top of the stack and visit it.
-> After we visit the last element 3, it doesn't have any unvisited adjacent nodes, so we have completed the Depth First Traversal of the graph.




DFS Pseudocode (recursive implementation):

-> The pseudocode for DFS is shown below. 

-> In the init() function, notice that we run the DFS function on every node. 
-> This is because the graph might have two different disconnected parts so to make sure that we cover every vertex, we can 
      also run the DFS algorithm on every node.


DFS(G, u)
 u.visited = true
 for each v ∈ G.Adj[u]
 if v.visited == false
 DFS(G,v)
 
init() {
 For each u ∈ G
 u.visited = false
 For each u ∈ G
 DFS(G, u)
}




Complexity of Depth First Search:

-> The time complexity of the DFS algorithm is represented in the form of O(V + E), 
      where V is the number of nodes and E is the number of edges.

-> The space complexity of the algorithm is O(V).




Application of DFS Algorithm:

-> For finding the path
-> To test if the graph is bipartite
-> For finding the strongly connected components of a graph
-> For detecting cycles in a graph





Notes:
1. Depth First Search -> Stack
2. Breadth First Search -> Queue
3. A graph is bipartite if the nodes can be partitioned into two independent sets A and B such that every edge in the graph 
      connects a node in set A and a node in set B . Return true if and only if it is bipartite.




Breadth first search:

-> Traversal means visiting all the nodes of a graph. 
-> Breadth First Traversal or Breadth First Search is a recursive algorithm for searching all the vertices of a graph or tree data structure.



BFS algorithm:
A standard BFS implementation puts each vertex of the graph into one of two categories:
1. Visited
2. Not Visited


-> The purpose of the algorithm is to mark each vertex as visited while avoiding cycles.




The algorithm works as follows:
1. Start by putting any one of the graph's vertices at the back of a queue.
2. Take the front item of the queue and add it to the visited list.
3. Create a list of that vertex's adjacent nodes. 
      Add the ones which aren't in the visited list to the back of the queue.
4. Keep repeating steps 2 and 3 until the queue is empty.
      
      
-> The graph might have two different disconnected parts so to make sure that we cover every vertex, we can also run the BFS algorithm on every node.



BFS example:

Let's see how the Breadth First Search algorithm works with an example. 

We use an undirected graph with 5 vertices.

-> We start from vertex 0, the BFS algorithm starts by putting it in the Visited list and putting all its adjacent vertices in the stack.
-> Next, we visit the element at the front of queue i.e. 1 and go to its adjacent nodes. Since 0 has already been visited, we visit 2 instead.
-> Vertex 2 has an unvisited adjacent vertex in 4, so we add that to the back of the queue and visit 3, which is at the front of the queue.
-> Visit 2 which was added to queue earlier to add its neighbours.
-> Only 4 remains in the queue since the only adjacent node of 3 i.e. 0 is already visited. We visit it.
-> Since the queue is empty, we have completed the Breadth First Traversal of the graph.



BFS pseudocode:

create a queue Q 
mark v as visited and put v into Q 
while Q is non-empty 
 remove the head u of Q 
 mark and enqueue all (unvisited) neighbours of u




BFS Algorithm Complexity:

-> The time complexity of the BFS algorithm is represented in the form of O(V + E), 
      where V is the number of nodes and E is the number of edges.
-> The space complexity of the algorithm is O(V).


BFS Algorithm Applications:
-> To build index by search index
-> For GPS navigation
-> Path finding algorithms
-> In Ford-Fulkerson algorithm to find maximum flow in a network
-> Cycle detection in an undirected graph
-> In minimum spanning tree


Notes:
-> Search engine indexing is the collecting, parsing, and storing of data to facilitate fast and accurate information retrieval.




Bellman Ford's Algorithm:

-> Bellman Ford algorithm helps us find the shortest path from a vertex to all other vertices of a weighted graph.
-> It is similar to Dijkstra's algorithm but it can work with graphs in which edges can have negative weights




How Bellman Ford's algorithm works?

-> Bellman Ford algorithm works by overestimating the length of the path from the starting vertex to all other vertices. 
-> Then it iteratively relaxes those estimates by finding new paths that are shorter than the previously overestimated paths.
-> By doing this repeatedly for all vertices, we can guarantee that the result is optimized.




Why would one ever have edges with negative weights in real life?

-> Negative weight edges might seem useless at first but they can explain a lot of phenomena like cashflow, the heat released/absorbed in a chemical reaction, etc.

-> For instance, if there are different ways to reach from one chemical A to another chemical B, each method will have sub-reactions involving both heat dissipation 
      and absorption.
-> If we want to find the set of reactions where minimum energy is required, then we will need to be able to factor in the heat absorption as negative weights and 
      heat dissipation as positive weights.




Why do we need to be careful with negative weights?

-> Negative weight edges can create negative weight cycles i.e. a cycle that will reduce the total path distance by coming back to the same point.


-> Shortest path algorithms like Dijkstra's Algorithm that aren't able to detect such a cycle can give an incorrect result 
      because they can go through a negative weight cycle and reduce the path length.




Bellman Ford Pseudocode:

-> We need to maintain the path distance of every vertex. 
-> We can store that in an array of size v, where v is the number of vertices.
-> We also want to be able to get the shortest path, not only know the length of the shortest path. 
-> For this, we map each vertex to the vertex that last updated its path length.
-> Once the algorithm is over, we can backtrack from the destination vertex to the source vertex to find the path.




function bellmanFord(G, S)
 for each vertex V in G
   distance[V] <- infinite
      previous[V] <- NULL
 distance[S] <- 0
 
 for each vertex V in G
   for each edge (U,V) in G
      tempDistance <- distance[U] + edge_weight(U, V)
      if tempDistance < distance[V]
         distance[V] <- tempDistance
         previous[V] <- U
 
 for each edge (U,V) in G
   If distance[U] + edge_weight(U, V) < distance[V}
   Error: Negative Cycle Exists
 
 return distance[], previous[]




Bellman Ford vs Dijkstra:

-> Bellman Ford's algorithm and Dijkstra's algorithm are very similar in structure. 
-> While Dijkstra looks only to the immediate neighbors of a vertex, Bellman goes through each edge in every iteration.




Bellman Ford's Complexity:

Time Complexity:

-> Best Case Complexity = O(E)
-> Average Case Complexity = O(VE)
-> Worst Case Complexity = O(VE)


Space Complexity:
-> And, the space complexity is O(V).




Bellman Ford's Algorithm Applications:

1. For calculating shortest paths in routing algorithms
2. For finding the shortest path.




Sorting and Searching Algorithms:

Stable Sorting Algorithms:

-> Stable sorting algorithms maintain the relative order of records with equal keys (i.e. values). 
-> That is, a sorting algorithm is stable if whenever there are two records R and S with the same key and with R appearing before S in the original list, 
      R will appear before S in the sorted list.
      
-> Several common sorting algorithms are stable by nature, such as Merge Sort, Timsort, Counting Sort, Insertion Sort, and Bubble Sort. 
      Others such as Quicksort, Heapsort and Selection Sort are unstable.  




Bubble Sort:

-> Bubble sort is a sorting algorithm that compares two adjacent elements and swaps them if they are not in the intended order.


Working of Bubble Sort:

Suppose we are trying to sort the elements in ascending order.

1. First Iteration (Compare and Swap)
-> Starting from the first index, compare the first and the second elements.
-> If the first element is greater than the second element, they are swapped.
-> Now, compare the second and the third elements. Swap them if they are not in order.
-> The above process goes on until the last element.


2. Remaining Iteration
-> The same process goes on for the remaining iterations.
-> After each iteration, the largest element among the unsorted elements is placed at the end.
-> In each iteration, the comparison takes place up to the last unsorted element.


-> The array is sorted when all the unsorted elements are placed at their correct positions.


Bubble Sort Algorithm:

BubbleSort(array)
 for i <- 1 to indexOfLastUnsortedElement-1
 if leftElement > rightElement
 swap leftElement and rightElement
end BubbleSort




// Bubble sort in Java

import java.util.Arrays;

class Main {
 // perform the bubble sort
 static void bubbleSort(int array[]) {
 int size = array.length;
 
 // loop to access each array element
 for (int i = 0; i < size - 1; i++)
 
   // loop to compare array elements
   for (int j = 0; j < size - i - 1; j++)
   // compare two adjacent elements
   // change > to < to sort in descending order
    if (array[j] > array[j + 1]) {
   // swapping occurs if elements
   // are not in the intended order
   int temp = array[j];
   array[j] = array[j + 1];
   array[j + 1] = temp;
   }
 }
 
 
 public static void main(String args[]) {
 
 int[] data = { -2, 45, 0, 11, -9 };
 
 // call method using class name
 Main.bubbleSort(data);
 
 System.out.println("Sorted Array in Ascending Order:");
 System.out.println(Arrays.toString(data));
 }
}




Optimized Bubble Sort Algorithm:

-> In the above algorithm, all the comparisons are made even if the array is already sorted.
-> This increases the execution time.

-> To solve this, we can introduce an extra variable swapped. 
-> The value of swappedis set true if there occurs swapping of elements. 
-> Otherwise, it is set false.

-> After an iteration, if there is no swapping, the value of swapped will be false. 
-> This means elements are already sorted and there is no need to perform further iterations.
-> This will reduce the execution time and helps to optimize the bubble sort.




Algorithm for optimized bubble sort:

BubbleSort(array)
 swapped <- false
 for i <- 1 to indexOfLastUnsortedElement-1
   if leftElement > rightElement
   swap leftElement and rightElement
   swapped <- true
end BubbleSort




// Optimized Bubble sort in Java

import java.util.Arrays;

class Main {
 // perform the bubble sort
 static void bubbleSort(int array[]) {
 int size = array.length;
 
 // loop to access each array element
 for (int i = 0; i < (size-1); i++) {
 
   // check if swapping occurs
    boolean swapped = false;
 
   // loop to compare adjacent elements
    for (int j = 0; j < (size-i-1); j++) {
 
   // compare two array elements
   // change > to < to sort in descending order
      if (array[j] > array[j + 1]) {
      // swapping occurs if elements
      // are not in the intended order
      int temp = array[j];
      array[j] = array[j + 1];
      array[j + 1] = temp;
 
      swapped = true;
   }
 }
 
 // no swapping means the array is already sorted
 // so no need for further comparison
 
 if (!swapped)
 break;
   }
 }


public static void main(String args[]) {
 
 int[] data = { -2, 45, 0, 11, -9 };
 
 // call method using the class name
 Main.bubbleSort(data);
 
 System.out.println("Sorted Array in Ascending Order:");
 System.out.println(Arrays.toString(data));
  }
}




Bubble Sort Complexity:

1. Time Complexities

• Worst Case Complexity: O(n^2)
If we want to sort in ascending order and the array is in descending order then the worst case occurs.

• Best Case Complexity: O(n)
If the array is already sorted, then there is no need for sorting.

• Average Case Complexity: O(n^2)
It occurs when the elements of the array are in jumbled order (neither ascending nor descending).



2. Space Complexity
• Space complexity is O(1) because an extra variable is used for swapping.
• In the optimized bubble sort algorithm, two extra variables are used. Hence, the space complexity will be O(2).


Stability of Bubble Sort Algorithm: Yes



Bubble Sort Applications:

-> Bubble sort is used if
• complexity does not matter
• short and simple code is preferred




Selection Sort Algorithm:

-> Selection sort is a sorting algorithm that selects the smallest element from an unsorted list in each iteration and 
      places that element at the beginning of the unsorted list.



Working of Selection Sort:
1. Set the first element as minimum.
2. Compare minimum with the second element. 
      If the second element is smaller than minimum, assign the second element as minimum.
      Compare minimum with the third element. 
      Again, if the third element is smaller, then assign minimum to the third element otherwise do nothing. 
      The process goes on until the last element.
3. After each iteration, minimum is placed in the front of the unsorted list.
4. For each iteration, indexing starts from the first unsorted element. 
      Step 1 to 3 are repeated until all the elements are placed at their correct positions.



Selection Sort Algorithm:

SelectionSort(array, size)
 repeat (size - 1) times
      set the first unsorted element as the minimum
      for each of the unsorted elements
            if element < currentMinimum
            set element as new minimum
      swap minimum with first unsorted position
end selectionSort




Time Complexities:
• Worst Case Complexity: O(n^2)
If we want to sort in ascending order and the array is in descending order then, the worst case occurs.

• Best Case Complexity: O(n2)
It occurs when the array is already sorted

• Average Case Complexity: O(n2)
It occurs when the elements of the array are in jumbled order (neither ascending nor descending).


-> The time complexity of the selection sort is the same in all cases. 
-> At every step, you have to find the minimum element and put it in the right place. 
-> The minimum element is not known until the end of the array is not reached.


Space Complexity:
Space complexity is O(1) because an extra variable temp is used.




Selection Sort Applications:

The selection sort is used when
• a small list is to be sorted
• cost of swapping does not matter
• checking of all the elements is compulsory
• cost of writing to a memory matters like in flash memory (number of writes/swaps is O(n) as compared to O(n^2) of bubble sort)




Insertion Sort Algorithm:

-> Insertion sort is a sorting algorithm that places an unsorted element at its suitable place in each iteration.

-> Insertion sort works similarly as we sort cards in our hand in a card game.
-> We assume that the first card is already sorted then, we select an unsorted card. 
-> If the unsorted card is greater than the card in hand, it is placed on the right otherwise, to the left. 
-> In the same way, other unsorted cards are taken and put in their right place.

-> A similar approach is used by insertion sort.



Working of Insertion Sort:

1. The first element in the array is assumed to be sorted. 
      Take the second element and store it separately in key.
      Compare key with the first element. 
      If the first element is greater than key, then key is placed in front of the first element.
2. Now, the first two elements are sorted.
      Take the third element and compare it with the elements on the left of it. 
      Placed it just behind the element smaller than it. 
      If there is no element smaller than it, then place it at the beginning of the array
3. Similarly, place every unsorted element at its correct position. 



Insertion Sort Algorithm:

insertionSort(array)
      mark first element as sorted
            for each unsorted element X
            'extract' the element X
            for j <- lastSortedIndex down to 0
                  if current element j > X
                  move sorted element to the right by 1
            break loop and insert X here
end insertionSort




Time Complexities:

• Worst Case Complexity: O(n^2)
-> Suppose, an array is in ascending order, and you want to sort it in descending order. 
-> In this case, worst case complexity occurs.
-> Each element has to be compared with each of the other elements so, for every nth element, (n-1) number of comparisons are made.
-> Thus, the total number of comparisons = n*(n-1) ~ n2

• Best Case Complexity: O(n)
-> When the array is already sorted, the outer loop runs for n number of times whereas the inner loop does not run at all. 
-> So, there are only n number of comparisons. 
-> Thus, complexity is linear.

• Average Case Complexity: O(n^2)
-> It occurs when the elements of an array are in jumbled order (neither ascending nor descending).




Space Complexity:

Space complexity is O(1) because an extra variable key is used.


Stability: Yes




Insertion Sort Applications:

The insertion sort is used when:
• the array is has a small number of elements
• there are only a few elements left to be sorted



Notes:
-> Both merge sort and quicksort employ a common algorithmic paradigm based on recursion. 
-> This paradigm, divide-and-conquer, breaks a problem into subproblems that are similar to the original problem, 
      recursively solves the subproblems, and finally combines the solutions to the subproblems to solve the original problem.



Divide and Conquer Strategy:

-> Using the Divide and Conquer technique, we divide a problem into subproblems. 
-> When the solution to each subproblem is ready, we 'combine' the results from the subproblems to solve the main problem.

-> Suppose we had to sort an array A. 
-> A subproblem would be to sort a sub-section of this array starting at index p and ending at index r, denoted as A[p..r].



Divide:
-> If q is the half-way point between p and r, then we can split the subarray A[p..r] into two arrays A[p..q] and A[q+1, r].

Conquer:
-> In the conquer step, we try to sort both the subarrays A[p..q] and A[q+1, r]. 
-> If we haven't yet reached the base case, we again divide both these subarrays and try to sort them.

Combine:
-> When the conquer step reaches the base step and we get two sorted subarrays A[p..q] and A[q+1, r] for array A[p..r], 
      we combine the results by creating a sorted array A[p..r] from two sorted subarrays A[p..q] and A[q+1, r].




Merge Sort Algorithm:

-> Merge Sort is one of the most popular sorting algorithms that is based on the principle of Divide and Conquer Algorithm.
-> Here, a problem is divided into multiple sub-problems. 
-> Each sub-problem is solved individually. 
-> Finally, sub-problems are combined to form the final solution.

-> The MergeSort function repeatedly divides the array into two halves until we reach a stage 
      where we try to perform MergeSort on a subarray of size 1 i.e. p == r.
-> After that, the merge function comes into play and combines the sorted arrays into larger arrays until the whole array is merged.


MergeSort(A, p, r):
      if p > r 
      return
      q = (p+r)/2
 mergeSort(A, p, q)
 mergeSort(A, q+1, r)
 merge(A, p, q, r)


-> To sort an entire array, we need to call MergeSort(A, 0, length(A)-1).



The merge Step of Merge Sort:

-> Every recursive algorithm is dependent on a base case and the ability to combineb the results from base cases. 
-> Merge sort is no different. 
-> The most important part of the merge sort algorithm is, you guessed it, merge step.
-> The merge step is the solution to the simple problem of merging two sorted lists(arrays) to build one large sorted list(array).


-> The algorithm maintains three pointers, one for each of the two arrays and one for maintaining the current index of the final sorted array.

Have we reached the end of any of the arrays?
 No:
 Compare current elements of both arrays. 
 Copy smaller element into sorted array.
 Move pointer of element containing smaller element.
 Yes:
 Copy all remaining elements of non-empty array




Writing the Code for Merge Algorithm:

-> A noticeable difference between the merging step we described above and the one we use for merge sort is that 
      we only perform the merge function on consecutive sub-arrays.
-> This is why we only need the array, the first position, the last index of the first subarray(we can calculate the first index of the second subarray) 
      and the last index of the second subarray.
-> Our task is to merge two subarrays A[p..q] and A[q+1..r] to create a sorted array A[p..r]. So the inputs to the function are A, p, q and r




The merge function works as follows:
1. Create copies of the subarrays L ← A[p..q] and M ← A[q+1..r].
2. Create three pointers i, j and k
      a. i maintains current index of L, starting at 1
      b. j maintains current index of M, starting at 1
      c. k maintains the current index of A[p..q], starting at p.
3. Until we reach the end of either L or M, pick the larger among the elements from L and M and place them in the correct position at A[p..q]
4. When we run out of elements in either L or M, pick up the remaining elements and put in A[p..q].




Time Complexity of Merge Sort Algorithms:
Best Case Complexity: O(n*log n)
Worst Case Complexity: O(n*log n)
Average Case Complexity: O(n*log n)


Space Complexity of Merge Sort Algorithms:
The space complexity of merge sort is O(n).


Stability: Yes




Merge Sort Applications:
• Inversion count problem
• External sorting
• E-commerce applications




Quicksort Algorithm:

-> Quicksort is a sorting algorithm based on the divide and conquer approach where
1. An array is divided into subarrays by selecting a pivot element (element selected from the array).
      While dividing the array, the pivot element should be positioned in such a way that elements less than pivot are kept on the left side 
      and elements greater than pivot are on the right side of the pivot.
2. The left and right subarrays are also divided using the same approach. 
      This process continues until each subarray contains a single element.
3. At this point, elements are already sorted. 
      Finally, elements are combined to form a sorted array.




Working of Quicksort Algorithm:
1. Select the Pivot Element
-> There are different variations of quicksort where the pivot element is selected from different positions. 
-> Here, we will be selecting the rightmost element of the array as the pivot element.

2. Rearrange the Array
-> Now the elements of the array are rearranged so that elements that are smaller than the pivot are put on the left 
      and the elements greater than the pivot are put on the right.




Here's how we rearrange the array:

1. A pointer is fixed at the pivot element. 
      The pivot element is compared with the elements beginning from the first index. 
2. If the element is greater than the pivot element, a second pointer is set for that element.
3. Now, pivot is compared with other elements. 
      If an element smaller than the pivot element is reached, the smaller element is swapped with the greater element found earlier.
4. Again, the process is repeated to set the next greater element as the second pointer. 
      And, swap it with another smaller element.
5. The process goes on until the second last element is reached.
6. Finally, the pivot element is swapped with the second pointer. 
7. Divide Subarrays.
8. Pivot elements are again chosen for the left and the right sub-parts separately. 
      And, step 2 is repeated.





Quick Sort Algorithm:

quickSort(array, leftmostIndex, rightmostIndex)
       if (leftmostIndex < rightmostIndex)
            pivotIndex <- partition(array,leftmostIndex, rightmostIndex)
quickSort(array, leftmostIndex, pivotIndex - 1)
quickSort(array, pivotIndex, rightmostIndex)


partition(array, leftmostIndex, rightmostIndex)
      set rightmostIndex as pivotIndex
      storeIndex <- leftmostIndex - 1
      for i <- leftmostIndex + 1 to rightmostIndex
            if element[i] < pivotElement
            swap element[i] and element[storeIndex]
      storeIndex++
 swap pivotElement and element[storeIndex+1]
return storeIndex + 1




1. Time Complexities:
• Worst Case Complexity: O(n^2)
-> It occurs when the pivot element picked is either the greatest or the smallest element.
-> This condition leads to the case in which the pivot element lies in an extreme end of the sorted array. 
-> One sub-array is always empty and another sub-array contains n - 1 elements. 
-> Thus, quicksort is called only on this sub-array.
-> However, the quicksort algorithm has better performance for scattered pivots.

• Best Case Complexity: O(n*log n)
It occurs when the pivot element is always the middle element or near to the middle element.

• Average Case Complexity: O(n*log n)
It occurs when the above conditions do not occur.


2. Space Complexity:
The space complexity for quicksort is O(log n).

Stability: No


Quicksort Applications:
Quicksort algorithm is used when
• the programming language is good for recursion
• time complexity matters
• space complexity matters




Notes:
-> Median-of-three partitioning is the best method for choosing an appropriate pivot element. 
      Picking a first, last or random element as a pivot is not much effective.
-> A quicksort algorithm should always aim to choose the middle-most element as its pivot. 
      Some algorithms will literally select the center-most item as the pivot, while others will select the first or the last element.



Searching Algorithms:

Linear Search:

-> Linear search is the simplest searching algorithm that searches for an element in a list in sequential order. 
-> We start at one end and check every element until the desired element is not found.




How Linear Search Works?

-> The following steps are followed to search for an element k = 1 in the list below.

1. Start from the first element, compare k with each element x.
2. If x == k, return the index.
3. Else, return not found.




Linear Search Algorithm:

LinearSearch(array, key)
 for each item in the array
      if item == value
 return its index


// Linear Search in Java
class LinearSearch {
 public static int linearSearch(int array[], int x) {
 int n = array.length;
 // Going through array sequencially
 for (int i = 0; i < n; i++) {
      if (array[i] == x)
      return i;
      }
      return -1;
 }
 
 
 
 public static void main(String args[]) {
 int array[] = { 2, 4, 0, 1, 9 };
 int x = 1;
 int result = linearSearch(array, x);
      if (result == -1)
      System.out.print("Element not found");
      else
      System.out.print("Element found at index: " + result);
      }
}




Linear Search Complexities:

Time Complexity: O(n)

Space Complexity: O(1)



Linear Search Applications:

1. For searching operations in smaller arrays (<100 items)




Binary Search:

-> Binary Search is a searching algorithm for finding an element's position in a sorted array.
-> In this approach, the element is always searched in the middle of a portion of an array.
-> Binary search can be implemented only on a sorted list of items. 
      If the elements are not sorted already, we need to sort them first.



Binary Search Working:

-> Binary Search Algorithm can be implemented in two ways which are discussed below.
1. Iterative Method
2. Recursive Method

-> The recursive method follows the divide and conquer approach.

The general steps for both methods are discussed below.
1. The array in which searching is to be performed is.
2. Set two pointers low and high at the lowest and the highest positions respectively.
3. Find the middle element mid of the array ie. arr[(low + high)/2]
4. If x == mid, then return mid.Else, compare the element to be searched with m.
5. If x > mid, compare x with the middle element of the elements on the right side of mid. 
      This is done by setting low to low = mid + 1.
6. Else, compare x with the middle element of the elements on the left side of mid. 
      This is done by setting high to high = mid - 1.
7. Repeat steps 3 to 6 until low meets high.




Binary Search Algorithm:

1. Iteration Method

do until the pointers low and high meet each other.
 mid = (low + high)/2
 if (x == arr[mid])
      return mid
 else if (x > arr[mid]) // x is on the right side
      low = mid + 1
 else // x is on the left side
      high = mid - 1


2. Recursive Method

binarySearch(arr, x, low, high)
 if low > high
      return False 
 else
      mid = (low + high) / 2 
 if x == arr[mid]
      return mid
 else if x > arr[mid]                                       // x is on the right side
      return binarySearch(arr, x, mid + 1, high)
 else                                                       // x is on the right side
      return binarySearch(arr, x, low, mid - 1)




Binary Search Complexity:

Time Complexities
• Best case complexity: O(1)
• Average case complexity: O(log n)
• Worst case complexity: O(log n)

Space Complexity
The space complexity of the binary search is O(1).




Binary Search Applications
• In libraries of Java, .Net, C++ STL
• While debugging, the binary search is used to pinpoint the place where the 
error happens.




Greedy Algorithm:

-> A greedy algorithm is an approach for solving a problem by selecting the best option available at the moment, 
      without worrying about the future result it would bring. 
-> In other words, the locally best choices aim at producing globally best results.


-> This algorithm may not be the best option for all the problems. 
      It may produce wrong results in some cases.
-> This algorithm never goes back to reverse the decision made. 
      This algorithm works in a top-down approach.




The main advantage of Greedy algorithm is:
1. The algorithm is easier to describe.
2. This algorithm can perform better than other algorithms (but, not in all cases).



Feasible Solution:

-> A feasible solution is the one that provides the optimal solution to the problem.

Greedy Algorithm:
1. To begin with, the solution set (containing answers) is empty.
2. At each step, an item is added into the solution set.
3. If the solution set is feasible, the current item is kept.
4. Else, the item is rejected and never considered again.


Example - Greedy Approach
Problem: You have to make a change of an amount using the smallest possible number of coins.

Amount: $28
Available coins:
 $5 coin
 $2 coin
 $1 coin



Solution:
1. Create an empty solution-set = { }.
2. coins = {5, 2, 1}
3. sum = 0
4. While sum ≠ 28, do the following.
5. Select a coin C from coins such that sum + C < 28.
6. If C + sum > 28, return no solution.
7. Else, sum = sum + C.
8. Add C to solution-set.


-> Up to the first 5 iterations, the solution set contains 5 $5 coins. 
      After that, we get 1 $2 coin and finally, 1 $1 coin.
      
      
      
Greedy Algorithm Applications:
• Selection Sort
• Knapsack Problem
• Minimum Spanning Tree
• Single-Source Shortest Path Problem
• Job Scheduling Problem
• Prim's Minimal Spanning Tree Algorithm
• Kruskal's Minimal Spanning Tree Algorithm
• Dijkstra's Minimal Spanning Tree Algorithm
• Huffman Coding
• Ford-Fulkerson Algorithm



Famous Algorithms:

1. Dijkstra's Algorithm:

-> Dijkstra's algorithm allows us to find the shortest path between any two vertices of a graph.
-> It differs from the minimum spanning tree because the shortest distance between two vertices might not include all the vertices of the graph.


How Dijkstra's Algorithm works?

-> Dijkstra's Algorithm works on the basis that any subpath B -> D of the shortest path A -> D between vertices 
      A and D is also the shortest path between vertices B and D.


-> Dijkstra used this property in the opposite direction i.e we overestimate the distance of each vertex from the starting vertex. 
-> Then we visit each node and its neighbors to find the shortest subpath to those neighbors.


-> The algorithm uses a greedy approach in the sense that we find the next best solution hoping that the end result is the best solution for the whole problem.




Djikstra's algorithm pseudocode:

-> We need to maintain the path distance of every vertex. 
-> We can store that in an array of size v, where v is the number of vertices.

-> We also want to be able to get the shortest path, not only know the length of the shortest path. 
-> For this, we map each vertex to the vertex that last updated its path length.

-> Once the algorithm is over, we can backtrack from the destination vertex to the source vertex to find the path.




Dijkstra's Algorithm Complexity:

Time Complexity: O(E Log V)
      where, E is the number of edges and V is the number of vertices.

Space Complexity: O(V)


Dijkstra's Algorithm Applications
• To find the shortest path
• In social networking applications
• In a telephone network
• To find the locations in the map




2. Kruskal's Algorithm:

-> Kruskal's algorithm is a minimum spanning tree algorithm that takes a graph as input and finds the subset of the edges of that graph which
• form a tree that includes every vertex
• has the minimum sum of weights among all the trees that can be formed from the graph




How Kruskal's algorithm works:

-> It falls under a class of algorithms called greedy algorithms that find the local optimum in the hopes of finding a global optimum.
-> We start from the edges with the lowest weight and keep adding edges until we reach our goal.


The steps for implementing Kruskal's algorithm are as follows:
1. Sort all the edges from low weight to high
2. Take the edge with the lowest weight and add it to the spanning tree. 
      If adding the edge created a cycle, then reject this edge.
3. Keep adding edges until we reach all vertices. 




Kruskal Algorithm Pseudocode:

-> Any minimum spanning tree algorithm revolves around checking if adding an edge creates a loop or not.
-> The most common way to find this out is an algorithm called Union FInd. 
-> The Union-Find algorithm divides the vertices into clusters and allows us to check if two vertices belong to the same cluster 
      or not and hence decide whether adding an edge creates a cycle.


KRUSKAL(G):
A = ∅
For each vertex v ∈ G.V:
 MAKE-SET(v)
For each edge (u, v) ∈ G.E ordered by increasing order by weight(u, v):
 if FIND-SET(u) ≠ FIND-SET(v): 
 A = A ∪ {(u, v)}
 UNION(u, v)
return A


Kruskal's Algorithm Complexity:

The time complexity Of Kruskal's Algorithm is: O(E log E).


Kruskal's Algorithm Applications:
• In order to layout electrical wiring
• In computer network (LAN connection)




Kruskal's vs Prim's Algorithm:

-> Prim's algorithm is another popular minimum spanning tree algorithm that uses a different logic to find the MST of a graph. 
-> Instead of starting from an edge, Prim's algorithm starts from a vertex and keeps adding lowest-weight edges which aren't 
      in the tree, until all vertices have been covered.




Prim's Algorithm:

-> Prim's algorithm is a minimum spanning tree algorithm that takes a graph as input and finds the subset of the edges of that graph which
• form a tree that includes every vertex
• has the minimum sum of weights among all the trees that can be formed from the graph




How Prim's algorithm works?

-> It falls under a class of algorithms called greedy algorithms that find the local optimum in the hopes of finding a global optimum.
-> We start from one vertex and keep adding edges with the lowest weight until we reach our goal.

The steps for implementing Prim's algorithm are as follows:
1. Initialize the minimum spanning tree with a vertex chosen at random.
2. Find all the edges that connect the tree to new vertices, find the minimum and add it to the tree
3. Keep repeating step 2 until we get a minimum spanning tree




Prim's Algorithm pseudocode:

-> The pseudocode for prim's algorithm shows how we create two sets of vertices U and V-U. 
-> U contains the list of vertices that have been visited and V-U the list of vertices that haven't. 

-> One by one, we move vertices from set V-U to set U by connecting the least weight edge.

T = ∅;
U = { 1 };
while (U ≠ V)
 let (u, v) be the lowest cost edge such that u ∈ U and v ∈ V - U;
 T = T ∪ {(u, v)}
 U = U ∪ {v}




Prim's Algorithm Complexity:

The time complexity of Prim's algorithm is O(E log V).

Prim's Algorithm Application
• Laying cables of electrical wiring
• In network designed
• To make protocols in network cycles




Huffman Coding:

-> Huffman Coding is a technique of compressing data to reduce its size without losing any of the details. 
-> It was first developed by David Huffman.
-> Huffman Coding is generally useful to compress the data in which there are frequently occurring characters.



How Huffman Coding works?

-> Suppose the string below is to be sent over a network.
-> Each character occupies 8 bits. 
-> There are a total of 15 characters in the string. 
-> Thus, a total of 8 * 15 = 120 bits are required to send this string.

-> Using the Huffman Coding technique, we can compress the string to a smaller size.
-> Huffman coding first creates a tree using the frequencies of the character and then generates code for each character.


-> Once the data is encoded, it has to be decoded. 
-> Decoding is done using the same tree.

-> Huffman Coding prevents any ambiguity in the decoding process using the concept of prefix code 
      ie. a code associated with a character should not be present in the prefix of any other code. 
-> The tree created above helps in maintaining the property.



Huffman coding is done with the help of the following steps.
1. Calculate the frequency of each character in the string.
2. Sort the characters in increasing order of the frequency. 
      These are stored in a priority queue Q.
3. Characters sorted according to the frequency 
4. Make each unique character as a leaf node.
5. Create an empty node z. 
      Assign the minimum frequency to the left child of z and assign the second minimum frequency to the right child of z. 
      Set the value of the z as the sum of the above two minimum frequencies.
6. Getting the sum of the least numbers 
7. Remove these two minimum frequencies from Q and add the sum into the list of frequencies (* denote the internal nodes in the figure above).
8. Insert node z into the tree.
9. Repeat steps 3 to 5 for all the characters.
10.Repeat steps 3 to 5 for all the characters.
11. For each non-leaf node, assign 0 to the left edge and 1 to the right edge.
12.For sending the above string over a network, we have to send the tree as well as the above compressed-code.




Decoding the code:
-> For decoding the code, we can take the code and traverse through the tree to find the character.



Huffman Coding Complexity:
-> The time complexity for encoding each unique character based on its frequency is O(nlog n).
-> Extracting minimum frequency from the priority queue takes place 2*(n-1) times and its complexity is O(log n). 
-> Thus the overall complexity is O(nlog n).



Huffman Coding Applications:
• Huffman coding is used in conventional compression formats like GZIP, BZIP2, PKZIP, etc.
• For text and fax transmissions




Dynamic Programming:

-> Dynamic Programming is a technique in computer programming that helps to efficiently solve a class of problems that have overlapping subproblems and 
      optimal substructure property.
      
      
      
Dynamic Programming Example:

-> Take the case of generating the fibonacci sequence.

-> If the sequence is F(1) F(2) F(3)........F(50), it follows the rule F(n) = F(n-1) + F(n-2)
F(50) = F(49) + F(48)
F(49) = F(48) + F(47)
F(48) = F(47) + F(46)
...

-> Notice how there are overlapping subproblems, we need to calculate F(48) to calculate both F(50) and F(49). 
-> This is exactly the kind of algorithm where Dynamic Programming shines.



How Dynamic Programming Works?

-> Dynamic programming works by storing the result of subproblems so that when their solutions are required, 
      they are at hand and we do not need to recalculate them.
-> This technique of storing the value of subproblems is called memorization. 
-> By saving the values in the array, we save time for computations of sub-problems we have already come across.


var m = map(0 → 0, 1 → 1)
function fib(n)
 if key n is not in map m 
 m[n] = fib(n − 1) + fib(n − 2)
 return m[n]


-> Dynamic programming by memorization is a top-down approach to dynamic programming. 
-> By reversing the direction in which the algorithm works i.e. by starting from the base case and working towards the solution, 
      we can also implement dynamic programming in a bottom-up manner.



function fib(n)
       if n = 0
            return 0
       else
            var prevFib = 0, currFib = 1
            repeat n − 1 times
            var newFib = prevFib + currFib
            prevFib = currFib
            currFib = newFib
 return currentFib



Recursion vs Dynamic Programming:

-> Dynamic programming is mostly applied to recursive algorithms. 
-> This is not a coincidence, most optimization problems require recursion and dynamic programming is used for optimization.

-> But not all problems that use recursion can use Dynamic Programming. 
-> Unless there is a presence of overlapping subproblems like in the fibonacci sequence problem, 
      a recursion can only reach the solution using a divide and conquer approach.

-> That is the reason why a recursive algorithm like Merge Sort cannot use Dynamic Programming, because the subproblems are not overlapping in any way


Greedy Algorithms vs Dynamic Programming:

-> Greedy Algorithms are similar to dynamic programming in the sense that they are both tools for optimization.

-> However, greedy algorithms look for locally optimum solutions or in other words, a greedy choice, in the hopes of finding a global optimum. 
-> Hence greedy algorithms can make a guess that looks optimum at the time but becomes costly down the line and do not guarantee a globally optimum.

-> Dynamic programming, on the other hand, finds the optimal solution to subproblems and then makes an informed choice to combine the results of those 
      subproblems to find the most optimum solution.





Backtracking Algorithm:

-> A backtracking algorithm is a problem-solving algorithm that uses a brute force approach for finding the desired output.
-> The Brute force approach tries out all the possible solutions and chooses the desired/best solutions.

-> The term backtracking suggests that if the current solution is not suitable, then backtrack and try other solutions. 
-> Thus, recursion is used in this approach.
-> This approach is used to solve problems that have multiple solutions. 
-> If you want an optimal solution, you must go for dynamic programming




State Space Tree:

-> A space state tree is a tree representing all the possible states (solution or nonsolution) of the problem 
      from the root as an initial state to the leaf as a terminal state.



Backtracking Algorithm:

Backtrack(x)
 if x is not a solution
      return false
 if x is a new solution
      add to list of solutions
 backtrack(expand x)



Example Backtracking Approach:

Problem: You want to find all the possible ways of arranging 2 boys and 1 girl on 3 benches. 
Constraint: Girl should not be on the middle bench.

Solution: There are a total of 3! = 6 possibilities. 
We will try all the possibilities and get the possible solutions. We recursively try all the possibilities.



Backtracking Algorithm Applications:
1. To find all Hamiltonian Paths present in a graph.
2. To solve the N Queen problem.
3. Maze solving problem.
4. The Knight's tour problem







































































